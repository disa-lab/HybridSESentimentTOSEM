{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"57cb25fd-5d58-462a-9f02-8bd2282509ba","_cell_guid":"f0657fd8-2d30-4bee-9225-4b1ae161a1d3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bert= BertForSequenceClassification.from_pretrained('bert-base-uncased')\n# print(bert.embeddings.word_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install bs4","metadata":{"_uuid":"9e38a2bd-2aef-40ac-abe2-06b707ce82d6","_cell_guid":"d5c8aa25-9fa0-458a-9131-24c94b4af45b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport glob\nimport random\nfrom tqdm import tqdm, trange\nimport time\n\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n\nfrom transformers import BertTokenizer\nfrom transformers import XLNetTokenizer\nfrom transformers import RobertaTokenizer\nfrom transformers import AlbertTokenizer\nfrom transformers import AdamW\n\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.preprocessing import OneHotEncoder\nfrom transformers import *\nimport warnings\n\n# from senti_utility import *  \nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"1e7a7fa7-7fe6-434d-b68c-971a04b5562e","_cell_guid":"b6ea7d83-7a1f-4f32-9104-345300b1921b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n\ntorch.cuda.empty_cache()","metadata":{"_uuid":"55f071c3-f883-400e-b594-dbbe4bbd67bf","_cell_guid":"33ce42a7-9684-4cc1-843a-eb5449751937","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters\nMAX_LEN=256\nBATCH_SIZE=16\nEPOCHS=4\nLEARNING_RATE=2e-5","metadata":{"_uuid":"c057cdd0-6777-4a2d-b773-60bf7d06f42f","_cell_guid":"8e37912a-b3d4-4c55-b734-2dc2662ce061","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODELS = [(BertForSequenceClassification,BertTokenizer,'bert-base-cased','bert'),\n          (XLNetForSequenceClassification, XLNetTokenizer,'xlnet-base-cased','xlnet'),\n          (RobertaForSequenceClassification, RobertaTokenizer,'roberta-base','Roberta'), \n          (AlbertForSequenceClassification, AlbertTokenizer,'albert-base-v1','albert')\n          ]\n\nMODEL_NAMES = ['bert', 'xlnet', 'Roberta', 'albert']","metadata":{"_uuid":"b74b868a-fbdd-4fec-b7a6-160c58fd535d","_cell_guid":"a8c953cb-6bd5-4ed7-a4b6-5a3ba27784e1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic=True\n\nseed_torch(42)","metadata":{"_uuid":"c87d83bf-710a-4691-99c6-c5bafdc3d3cc","_cell_guid":"9ea976ab-2bab-4fee-93d2-067ef1e5c6ec","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df  = pd.read_csv(\"../input/sentiment-paper-s4/ResultsConsolidatedWithEnsambleAssessment_BERTLabeled.csv\")\ndf['ManualLabel']=df['ManualLabel'].replace({'o': 0, 'p': 1,'n':2})\nfinal_pred = pd.Series()\ndfs = []","metadata":{"_uuid":"37b4c253-3722-47e3-a7ea-55efda7a9e9f","_cell_guid":"ef363373-5057-4467-89ff-681d4aea8a22","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"_uuid":"32e9e659-fcea-4143-8ba5-6782ad2f5cbd","_cell_guid":"6105f926-dc40-4bb1-805d-2ff1a0348fc4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"JIRA = df[df['File'].str.contains('LinJIRA')]","metadata":{"_uuid":"b27fecc2-da1d-4e52-abd1-3c69418f9e36","_cell_guid":"7eead3f0-c4d6-482e-8113-78b889ef9898","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BOGIAS = df[df['File'].str.contains('BenchmarkUddinSO')]\nLINAPP = df[df['File'].str.contains('DatasetLinAppReviews')]\nLINSO = df[df['File'].str.contains('DatasetLinSO')]\nSENTI4S = df[df['File'].str.contains('DatasetSenti4SDSO')]\nORTUJIRA = df[df['File'].str.contains('OrtuJIRA')]","metadata":{"_uuid":"e0743286-cf34-475f-b681-53aa1ab4f4f7","_cell_guid":"fcf9e908-d1b5-4b6c-9bcb-7b5c1b6e5834","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = LINAPP\n# df = ORTUJIRA","metadata":{"_uuid":"252a6d03-cb39-4eae-b9d6-e3c342b6b403","_cell_guid":"21526e31-6f84-427f-a201-035d981ab9c5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport pandas as pd \nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\nfrom imblearn.over_sampling import SMOTE\nfrom statistics import mean\nimport numpy as np\nimport argparse\nimport csv\nimport scipy as sp\nfrom scipy.sparse import coo_matrix, hstack\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom nltk.stem.snowball import SnowballStemmer\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import SVMSMOTE\nimport math\nfrom nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer\nfrom nltk.corpus import stopwords\nimport re\nfrom bs4 import BeautifulSoup\nfrom multiprocessing import Process\n\nstopWords = set(stopwords.words('english'))\nstemmer =SnowballStemmer(\"english\")\n\n\nmystop_words=[\n'i', 'me', 'my', 'myself', 'we', 'our',  'ourselves', 'you', 'your',\n'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her',\n'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'themselves',\n 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n'and',  'if', 'or', 'as', 'until',  'of', 'at', 'by',  'between', 'into',\n'through', 'during', 'to', 'from', 'in', 'out', 'on', 'off', 'then', 'once', 'here',\n 'there',  'all', 'any', 'both', 'each', 'few', 'more',\n 'other', 'some', 'such',  'than', 'too', 'very', 's', 't', 'can', 'will',  'don', 'should', 'now'\n# keywords\n 'while', 'case', 'switch','def', 'abstract','byte','continue','native','private','synchronized',\n 'if', 'do', 'include', 'each', 'than', 'finally', 'class', 'double', 'float', 'int','else','instanceof',\n 'long', 'super', 'import', 'short', 'default', 'catch', 'try', 'new', 'final', 'extends', 'implements',\n 'public', 'protected', 'static', 'this', 'return', 'char', 'const', 'break', 'boolean', 'bool', 'package',\n 'byte', 'assert', 'raise', 'global', 'with', 'or', 'yield', 'in', 'out', 'except', 'and', 'enum', 'signed',\n 'void', 'virtual', 'union', 'goto', 'var', 'function', 'require', 'print', 'echo', 'foreach', 'elseif', 'namespace',\n 'delegate', 'event', 'override', 'struct', 'readonly', 'explicit', 'interface', 'get', 'set','elif','for',\n 'throw','throws','lambda','endfor','endforeach','endif','endwhile','clone',\n u\"'d\", u\"'s\", u'abov', u'ani', u'becaus', u'befor', u'continu', u'could', u'deleg', u'doe', u'doubl', u'dure', \n u'els', u'endwhil', u'extend', u'implement', u'includ', u'interfac', u'might', u'must', u\"n't\", u'namespac', \n u'nativ', u'need', u'nowwhil', u'onc', u'onli', u'ourselv', u'overrid', u'packag', u'privat', u'protect', u'rais', \n u'readon', u'requir', u'sha', u'sign', u'synchron', u'themselv', u'tri', u'veri', u'whi', u'wo', u'would', u'yourselv',\n u'el', u'rai'\n]\n\ninfileContractions = \"../input/sentiment-auxilary/Contractions.txt\"\ninfileEmotionLookup = \"../input/sentiment-auxilary/EmoticonLookupTable.txt\"\n\nemodict=[]\ncontractions_dict=[]\n# Read in the words with sentiment from the dictionary\nwith open(infileContractions,\"r\") as contractions,\\\n     open(infileEmotionLookup,\"r\") as emotable:\n    contractions_reader=csv.reader(contractions, delimiter='\\t')\n    emoticon_reader=csv.reader(emotable,delimiter='\\t')\n\n    #Hash words from dictionary with their values\n    contractions_dict = {rows[0]:rows[1] for rows in contractions_reader}\n    emodict={rows[0]:rows[1] for rows in emoticon_reader}\n\n    contractions.close()\n    emotable.close()\n\n\n\ngrammar= r\"\"\"\nNegP: {<VERB>?<ADV>+<VERB|ADJ>?<PRT|ADV><VERB>}\n{<VERB>?<ADV>+<VERB|ADJ>*<ADP|DET>?<ADJ>?<NOUN>?<ADV>?}\n\n\"\"\"\nchunk_parser = nltk.RegexpParser(grammar)\ncontractions_regex = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\ndef expand_contractions(s, contractions_dict=contractions_dict):\n     def replace(match):\n        return contractions_dict[match.group(0)]\n     return contractions_regex.sub(replace, s.lower())\n\n\nurl_regex = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n\ndef remove_url(s):\n    return url_regex.sub(\" \",s)\n\nnegation_words =['not', 'never', 'none', 'nobody', 'nowhere', 'neither', 'barely', 'hardly',\n                     'nothing', 'rarely', 'seldom', 'despite' ]\n\nemoticon_words=['PositiveSentiment','NegativeSentiment']\n\ndef negated(input_words):\n    \"\"\"\n    Determine if input contains negation words\n    \"\"\"\n    neg_words = []\n    neg_words.extend(negation_words)\n    for word in neg_words:\n        if word in input_words:\n            return True\n    return False\n\ndef prepend_not(word):\n    if word in emoticon_words:\n        return word\n    elif word in negation_words:\n        return word\n    return \"NOT_\"+word\n\ndef handle_negation(comments):\n    sentences = nltk.sent_tokenize(comments)\n    modified_st=[]\n    for st in sentences:\n        allwords = nltk.word_tokenize(st)\n#         print(allwords)\n        modified_words=[]\n        if negated(allwords):\n            part_of_speech = nltk.tag.pos_tag(allwords,tagset='universal')\n            chunked = chunk_parser.parse(part_of_speech)\n            print(chunked)\n            #print(\"---------------------------\")\n            #print(st)\n            for n in chunked:\n                if isinstance(n, nltk.tree.Tree):\n                    words = [pair[0] for pair in n.leaves()]\n                    #print(words)\n\n                    if n.label() == 'NegP' and negated(words):\n                        for i, (word, pos) in enumerate(n.leaves()):\n                            if (pos==\"ADV\" or pos==\"ADJ\" or pos==\"VERB\") and (word!=\"not\"):\n                                modified_words.append(prepend_not(word))\n                            else:\n                                modified_words.append(word)\n                    else:\n                         modified_words.extend(words)\n                else:\n                    modified_words.append(n[0])\n            newst =' '.join(modified_words)\n            #print(newst)\n            modified_st.append(newst)\n        else:\n            modified_st.append(st)\n    return \". \".join(modified_st)\n\npat1 = r'@[A-Za-z0-9_]+'\npat2 = r'https?://[^ ]+'\ncombined_pat = r'|'.join((pat1, pat2))\nwww_pat = r'www.[^ ]+'\nnegations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n                \"mustn't\":\"must not\"}\nneg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\ntok = WordPunctTokenizer()\n\n\nfor w in mystop_words:\n    stopWords.add(w)\nstopWords = list(stopWords)\n\ndef stem_tokens(tokens):\n    stemmed = []\n    for item in tokens:\n        stemmed.append(stemmer.stem(item))\n    return stemmed\n\ndef tokenize_and_stem(text):\n    tokens = nltk.word_tokenize(text)\n    stems = stem_tokens(tokens)\n    #stems = tokens\n    return stems\n\ndef replace_all(text, dic):\n    for i, j in dic.items():\n        text = text.replace(i, j)\n    return text\n\ndef preprocess_text(text):\n    if text is None: return []\n    #comments = text.encode('ascii', 'ignore')\n    text = text.lower()\n    text = text.replace(\"\\\\\", \"\")\n    comments = text\n    comments = expand_contractions(comments)\n    comments = remove_url(comments)\n#     print(emodict)\n    comments = replace_all(comments, emodict)\n    #comments = tweet_cleaner_updated(comments)\n    #comments = Utils.handle_negation(comments)\n    #comments = detectSentimentDominentClause(comments)    return  comments\n    return comments","metadata":{"_uuid":"9264b477-6a39-4220-87ce-d6ec056f31dc","_cell_guid":"08e86a6b-8ba0-42f3-8d3e-ca20b54de567","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_prediction={}\nfor j in range(1,2):\n    \n    m_num=j\n    cur_model=MODELS[m_num]\n    print(cur_model[3])\n    m_name=MODEL_NAMES[m_num]\n    df_model = pd.DataFrame()\n    \n    df_prediction[m_name] = []\n    cross_fold = 10\n    for k in range(cross_fold):\n        test_df  = df[df['File'].str.contains(\"_{}\".format(k))]\n        train_df  = df[~df['File'].str.contains(\"_{}\".format(k))]\n#         print(df, test_df)\n        tokenizer = cur_model[1].from_pretrained(cur_model[2], do_lower_case=True)\n        old_tokenizer_size = len(tokenizer)\n        new_token = ['senti4s_o', 'senti4s_n','senti4s_p','senticr_o','senticr_p','senticr_n','sentistrength_o', 'sentistrength_n', 'sentistrength_p','pome_o','pome_n','pome_p','dsolabel_o','dsolabel_n','dsolabel_p']\n        tokenizer.add_tokens(new_token)\n        sentences=train_df.Sentence.values\n        labels=train_df.ManualLabel.values\n        senti=train_df.Senti4SD.values\n        senticr=train_df.SentiCR.values\n        sentistr=train_df.SentistrengthSE.values\n        pome=train_df.POME.values\n        dso = train_df.DsoLabelFullText.values\n#         numericuls = train_df[['Senti4SD','SentiCR','SentistrengthSE','POME','DsoLabelFullText', 'BERT', 'ROBERTA', 'XLNET','ALBERT']].to_numpy()\n        \n        input_ids = []\n        attention_masks = []\n        input_numerciculs=[]\n#         enc = OneHotEncoder(handle_unknown='ignore')\n#         enc.fit(numericuls)\n#         for sent,num in zip(sentences,numericuls):\n        for sent,l1, l2, l3, l4, l5 in zip(sentences, senti, senticr, sentistr, pome, dso):\n            sent = str(sent)\n            if len(sent)==0:\n                sent = \"null\"\n            #print(sent)\n            sent = sent+ \".senti4s_{}.senticr_{}.sentistrength_{}.pome_{}.dsolabel_{}.\".format(l1,l2,l3,l4,l5)\n            psent = preprocess_text(sent)\n#             print(psent)\n            encoded_dict = tokenizer.encode_plus(\n                                str(psent), \n                                add_special_tokens = True, \n                                max_length = MAX_LEN,\n                                pad_to_max_length = True,\n                                return_attention_mask = True, \n                                return_tensors = 'pt',\n                                truncation=True\n                            )\n#             print(encoded_dict)\n#             break\n            input_ids.append(encoded_dict['input_ids'])\n            attention_masks.append(encoded_dict['attention_mask'])\n#             num = np.array(num).reshape(1,9)\n#             input_numerciculs.append(torch.from_numpy(enc.transform(num).toarray()))\n        train_inputs = torch.cat(input_ids, dim=0)\n        train_masks = torch.cat(attention_masks, dim=0)\n        train_labels = torch.tensor(labels)\n#         train_numericul = torch.cat(input_numerciculs, dim=0)\n        \n        print('Training data {} {} {}'.format(train_inputs.shape, train_masks.shape, train_labels.shape))\n        \n        \n#         train_data = TensorDataset(train_inputs, train_masks, train_labels,train_numericul)\n        train_data = TensorDataset(train_inputs, train_masks, train_labels)\n        train_sampler = RandomSampler(train_data)\n        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n        \n        # Train Model\n#         cur_model[0].num_size = train_numericul.shape[1]\n        model = cur_model[0].from_pretrained(cur_model[2], num_labels=3)\n        \n        model.resize_token_embeddings(len(tokenizer)) \n#         print(model.word_embeddings)\n#         for idx in range(len(new_token)):\n#             model.embeddings.word_embeddings.weight[old_tokenizer_size+idx, :] = torch.zeros([model.config.hidden_size])\n        \n        model.cuda()\n        \n        param_optimizer = list(model.named_parameters())\n        no_decay = ['bias', 'gamma', 'beta']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n              'weight_decay_rate': 0.01},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n              'weight_decay_rate': 0.0}\n        ]\n        \n        optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n        \n        begin=time.time()\n        train_loss_set = []\n        \n        for _ in trange(EPOCHS, desc=\"Epoch\"): \n            model.train()\n        \n            tr_loss = 0\n            nb_tr_examples, nb_tr_steps = 0, 0\n            \n            for step, batch in enumerate(train_dataloader):\n            \n                batch = tuple(t.to(device) for t in batch)\n              \n#                 b_input_ids, b_input_mask, b_labels, b_input_numericul = batch\n                b_input_ids, b_input_mask, b_labels = batch\n                optimizer.zero_grad()\n                # Forward pass\n                outputs = model(b_input_ids, token_type_ids=None, \\\n                                attention_mask=b_input_mask, labels=b_labels ) #b_input_ids, b_input_mask, b_labels, b_input_numericul = batch\n                loss = outputs[0]\n                logits = outputs[1]\n                train_loss_set.append(loss.item())    \n                \n                # Backward pass\n                loss.backward()\n                optimizer.step()\n                \n                tr_loss += loss.item()\n                nb_tr_examples += b_input_ids.size(0)\n                nb_tr_steps += 1\n        \n            print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n        \n        end=time.time()\n        print('Training used {} second'.format(end-begin))\n        \n        # Test Model\n        begin=time.time()    \n        sentences=test_df.Sentence.values\n        labels = test_df.ManualLabel.values\n#         numericuls = test_df[['Senti4SD','SentiCR','SentistrengthSE','POME','DsoLabelFullText', 'BERT', 'ROBERTA', 'XLNET','ALBERT']].to_numpy()\n        input_ids = []\n        attention_masks = []\n        senti=test_df.Senti4SD.values\n        senticr=test_df.SentiCR.values\n        sentistr=test_df.SentistrengthSE.values\n        pome=test_df.POME.values\n        dso = test_df.DsoLabelFullText.values\n        \n#         input_numerciculs=[]\n        \n#         for sent ,num in zip(sentences,numericuls):\n        for sent, l1, l2,l3,l4,l5 in zip(sentences, senti, senticr, sentistr, pome, dso):\n            sent = str(sent)\n            if len(sent)==0:\n                sent = \"null\"    \n            sent = sent+ \".senti4s_{}.senticr_{}.sentistrength_{}.pome_{}.dsolabel_{}.\".format(l1,l2,l3,l4,l5)\n            psent = preprocess_text(sent)\n            encoded_dict = tokenizer.encode_plus(\n                            str(psent), \n                            add_special_tokens = True, \n                            max_length = MAX_LEN,\n                            pad_to_max_length = True,\n                            return_attention_mask = True, \n                            return_tensors = 'pt'\n                            )\n             \n            input_ids.append(encoded_dict['input_ids'])\n            attention_masks.append(encoded_dict['attention_mask'])\n#             num = np.array(num).reshape(1,9)\n#             input_numerciculs.append(torch.from_numpy(enc.transform(num).toarray()))\n        prediction_inputs = torch.cat(input_ids,dim=0)\n        prediction_masks = torch.cat(attention_masks,dim=0)\n        prediction_labels = torch.tensor(labels)\n#         prediction_numericul = torch.cat(input_numerciculs, dim=0)\n        \n        prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels) # ,prediction_numericul\n        prediction_sampler = SequentialSampler(prediction_data)\n        prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=BATCH_SIZE)\n        \n        model.eval()\n        predictions,true_labels=[],[]\n        \n        for batch in prediction_dataloader:\n            batch = tuple(t.to(device) for t in batch)\n#             b_input_ids, b_input_mask, b_labels,b_input_numericul = batch\n            b_input_ids, b_input_mask, b_labels = batch\n            \n        \n            with torch.no_grad():\n                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) #,input_numericul=b_input_numericul\n                logits = outputs[0]\n        \n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n            \n            predictions.append(logits)\n            true_labels.append(label_ids)\n            \n        end=time.time()\n        print('Prediction used {:.2f} seconds'.format(end-begin))\n        \n        flat_predictions = [item for sublist in predictions for item in sublist]\n        flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n        flat_true_labels = [item for sublist in true_labels for item in sublist]\n        \n        print(\"Accuracy of {} on Jira is: {}\".format(m_name, accuracy_score(flat_true_labels,flat_predictions)))\n        print(classification_report(flat_true_labels, flat_predictions))\n        print(flat_true_labels)\n        test_df[cur_model[3]] = flat_predictions\n        df_model = df_model.append(test_df)\n        torch.cuda.empty_cache()\n#         print(len(flat_predictions))\n        df_prediction[m_name].extend(flat_predictions)","metadata":{"_uuid":"e6cee41a-f1dd-44ca-a2dc-76b68221c2f8","_cell_guid":"b35d4d25-4647-43c8-80bd-4c94c2b38fc3","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bert = []\n# for d in df_prediction['bert']:\n#     bert.extend(d)\n# roberta = []\n# for d in df_prediction['Roberta']:\n#     roberta.extend(d)\n# xlnet = []\n# for d in df_prediction['xlnet']:\n#     xlnet.extend(d)\n# albert = []\n# for d in df_prediction['albert']:\n#     albert.extend(d)","metadata":{"_uuid":"5676f8cc-2774-4fec-9846-310eeb7b45ed","_cell_guid":"5ffd9678-8d20-4210-9f9e-b0ec5fc94a37","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_bogias = {'bert':bert, 'roberta': roberta, 'xlnet': xlnet, 'albert': albert}","metadata":{"_uuid":"9081243c-98a5-4990-b2d6-8f325884a330","_cell_guid":"d1ca8cff-09dc-44ff-a551-356b56ec274b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pickle","metadata":{"_uuid":"7832d4f1-9ac7-4060-ab90-4cc5223eb9c5","_cell_guid":"2becf841-d57a-41a6-ba2a-6a1871c09a3d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LINAPP['bert-1'] = df_prediction['bert']\n# LINAPP['roberta-1'] = df_prediction['Roberta']\n# LINAPP['xlnet-1'] = df_prediction['xlnet']\n# LINAPP['albert-1'] = df_prediction['albert']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# LINAPP['bert-1'] = LINAPP['bert-1'].replace({1:'p',2:'n', 0:'o'})\n# LINAPP['xlnet-1'] = LINAPP['xlnet-1'].replace({1:'p',2:'n', 0:'o'})\n# LINAPP['albert-1'] = LINAPP['albert-1'].replace({1:'p',2:'n', 0:'o'})\n# LINAPP['roberta-1'] = LINAPP['roberta-1'].replace({1:'p',2:'n', 0:'o'})\n# LINAPP['ManualLabel'] = LINAPP['ManualLabel'].replace({1:'p',2:'n', 0:'o'})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LINAPP.to_csv('test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pickle.dump(df_bogias, open('linapp_run3.p','wb'))","metadata":{"_uuid":"bb4f57c7-2207-4a76-9829-116162888b42","_cell_guid":"3afd1e7d-a6fc-4bbe-9279-69b83bb65429","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}